{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff2b5d5-26ab-4850-aa0a-81a52d3f031e",
   "metadata": {},
   "source": [
    "# Climate projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f495ab8-501b-4124-b425-974d224a0ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import os \n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "\n",
    "# climate-related\n",
    "import intake\n",
    "import gcsfs\n",
    "import cftime\n",
    "from xclim import sdba\n",
    "from xclim import set_options\n",
    "\n",
    "# others\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#bug in specific gcm\n",
    "#from xmip.preprocessing import combined_preprocessing\n",
    "\n",
    "chunks_dict = {\"lon\": 10, \"lat\": 10, \"time\": -1}\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\") \n",
    "\n",
    "# check what gcm to use (not only generic list)\n",
    "gcm_list  = [\"ACCESS-CM2\", \"BCC-CSM2-MR\", \"CMCC-ESM2\", \"FGOALS-f3-L\", \"GFDL-ESM4\", \"CMCC-CM2-SR5\", \"KACE-1-0-G\", \"MPI-ESM1-2-HR\", \"MRI-ESM2-0\", \"MIROC6\"]\n",
    "ssp_list  = [\"ssp126\",\"ssp245\",\"ssp370\", \"ssp585\"] \n",
    "climate   = [\"ERA5\", \"MSWEP\", \"PMET\", \"CR2MET\"]\n",
    "bias_correction = [\"MVA\",\"DQM\",\"MBC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e53dc91-9e70-4574-921b-7ceee184c7c3",
   "metadata": {},
   "source": [
    "## 1. Download and preprocess selected GCMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eb6be2-9a88-48d3-9977-d31eecc6b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\n",
    "dataframe = intake.open_esm_datastore(url)\n",
    "\n",
    "dataframe = dataframe.search(experiment_id = ['historical'] + ssp_list, # scenarios  \n",
    "                             table_id      = 'Amon', # time-step\n",
    "                             variable_id   = ['tas', 'pr'], # variables\n",
    "                             member_id     = 'r1i1p1f1', # member\n",
    "                             source_id     = gcm_list)  # models\n",
    "\n",
    "datasets = dataframe.to_dataset_dict()\n",
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea06bb-942e-4b06-ba7f-1c2c8752155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "period = slice(\"1980-01-01\", \"2099-12-25\") # period\n",
    "days = np.array([31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31])\n",
    "\n",
    "lat_coords = np.arange(-56,-40, 0.5)\n",
    "lon_coords = np.arange(-75,-67, 0.5)\n",
    "\n",
    "for gcm in tqdm(gcm_list):\n",
    "    gcm_historical  = next(val for key, val in datasets.items() if gcm + \".historical\" in key)\n",
    "    \n",
    "    if isinstance(gcm_historical.indexes['time'][0], cftime.DatetimeProlepticGregorian) or isinstance(gcm_historical.indexes['time'][0], cftime.Datetime360Day) or isinstance(gcm_historical.indexes['time'][0], cftime.DatetimeNoLeap):\n",
    "        gcm_historical[\"time\"] = gcm_historical.indexes['time'].to_datetimeindex() # fix the time dimension if its necessary\n",
    "    \n",
    "    for ssp in tqdm(ssp_list):\n",
    "        \n",
    "        gcm_ssp = next(val for key, val in datasets.items() if gcm + \".\" + ssp in key)\n",
    "        gcm_ssp = gcm_ssp.sel(time=period) # fix for 2100-2300\n",
    "        \n",
    "        if isinstance(gcm_ssp.indexes['time'][0], cftime.DatetimeProlepticGregorian) or isinstance(gcm_ssp.indexes['time'][0], cftime.Datetime360Day) or isinstance(gcm_ssp.indexes['time'][0], cftime.DatetimeNoLeap):\n",
    "            gcm_ssp[\"time\"] = gcm_ssp.indexes['time'].to_datetimeindex() # fix the time dimension if its necessary\n",
    "        \n",
    "        gcm_ssp = xr.concat([gcm_historical, gcm_ssp], dim = \"time\")\n",
    "        gcm_ssp = gcm_ssp.drop([\"height\", \"dcpp_init_year\", \"member_id\", \"lat_bnds\", \"lon_bnds\", \"time_bnds\"])\n",
    "        gcm_ssp = gcm_ssp.sel(member_id=0, drop=True).sel(dcpp_init_year=0, drop=True)\n",
    "        gcm_ssp.coords['lon'] = (gcm_ssp.coords['lon'] + 180) % 360 - 180\n",
    "        gcm_ssp = gcm_ssp.sortby(gcm_ssp.lon)\n",
    "        gcm_ssp = gcm_ssp.interp(lat = lat_coords, lon = lon_coords)\n",
    "        gcm_ssp = gcm_ssp.sel(time=period)\n",
    "        \n",
    "        gcm_ssp[\"tas\"] = gcm_ssp.tas - 273.15 # to degC\n",
    "        months  = xr.DataArray(np.tile(days, 2099-1980+1), coords=[gcm_ssp.time], name='month_length')\n",
    "        gcm_ssp[\"pr\"]  = (gcm_ssp.pr*months*84600) # to mm month-1\n",
    "        gcm_ssp[\"pr\"]  = gcm_ssp.pr.astype(\"int16\")\n",
    "\n",
    "        # change units\n",
    "        gcm_ssp.pr.attrs[\"units\"] = \"mm\"\n",
    "        gcm_ssp.tas.attrs[\"units\"] = \"degC\"\n",
    "        \n",
    "        gcm_ssp.tas.to_netcdf(\"/home/rooda/OGGM_results/Future_climate/T2M_\" + gcm + \"_\" + ssp + \".nc\")\n",
    "        gcm_ssp.pr.to_netcdf(\"/home/rooda/OGGM_results/Future_climate/PP_\"   + gcm + \"_\" + ssp + \".nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62dbaba-7872-4f6f-8ecc-c1c7e0f9f389",
   "metadata": {},
   "source": [
    "## 2. Bias corrections methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e8aaa-d87e-4ff9-b5b4-4f5b0ee8a4a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks_dict = {\"lon\": 20, \"lat\": 20, \"time\": -1}\n",
    "encode_t2m  = {'tas': {'dtype': 'int16', 'scale_factor': 0.01, '_FillValue': -9999}}\n",
    "encode_pp   = {\"pr\":  {'dtype': 'int16', 'zlib': True, 'complevel': 1}}\n",
    "\n",
    "for file_id in tqdm(climate): \n",
    "    \n",
    "    future_period   = slice(\"2020-01-01\", \"2099-12-31\") # Future period to bias correct\n",
    "    baseline_period = slice(\"1980-01-01\", \"2014-12-31\") # ISIMIP3b bias adjustment protocol\n",
    "\n",
    "    pp_baseline  = xr.open_dataset(\"/home/rooda/OGGM_results/\" + file_id + \"_OGGM_1980_2019m.nc\")[\"prcp\"]\n",
    "    t2m_baseline = xr.open_dataset(\"/home/rooda/OGGM_results/\" + file_id + \"_OGGM_1980_2019m.nc\")[\"temp\"]\n",
    "    pp_baseline  = pp_baseline.sel(time  = baseline_period)\n",
    "    t2m_baseline = t2m_baseline.sel(time = baseline_period)\n",
    "\n",
    "    os.chdir(\"/home/rooda/OGGM_results/Future_climate/\")\n",
    "\n",
    "    for gcm in tqdm(gcm_list, leave=True):\n",
    "        for ssp in tqdm(ssp_list, leave = False):\n",
    "            pp_model_ssp = xr.open_dataset(\"PP_\" + gcm + \"_\" + ssp + \".nc\")[\"pr\"]\n",
    "            pp_model_ssp = pp_model_ssp.interp(lat = pp_baseline.lat, lon = pp_baseline.lon)\n",
    "            pp_model_ssp = pp_model_ssp.where(pp_baseline[0].notnull())\n",
    "            pp_model_ssp = pp_model_ssp.chunk(chunks_dict)\n",
    "\n",
    "            t2m_model_ssp = xr.open_mfdataset(\"T2M_\" + gcm + \"_\" + ssp + \".nc\")[\"tas\"]\n",
    "            t2m_model_ssp = t2m_model_ssp.interp(lat = t2m_baseline.lat, lon = t2m_baseline.lon)\n",
    "            t2m_model_ssp = t2m_model_ssp.where(t2m_baseline[0].notnull())\n",
    "            t2m_model_ssp = t2m_model_ssp.chunk(chunks_dict)\n",
    "\n",
    "            for bc in tqdm(bias_correction, leave = False): \n",
    "                if bc == \"MVA\": # Scaling method\n",
    "                    qdm_t2m = sdba.adjustment.Scaling.train(ref = t2m_baseline, hist = t2m_model_ssp.sel(time = baseline_period), kind = \"+\", group = \"time.month\")\n",
    "                    t2m_model_ssp_bc = qdm_t2m.adjust(t2m_model_ssp.sel(time = future_period), interp=\"nearest\")      \n",
    "\n",
    "                    qdm_pp  = sdba.adjustment.Scaling.train(ref = pp_baseline, hist = pp_model_ssp.sel(time  = baseline_period), kind = \"*\", group = \"time.month\")\n",
    "                    pp_model_ssp_bc  = qdm_pp.adjust(pp_model_ssp.sel(time  = future_period), interp=\"nearest\")\n",
    "                \n",
    "                if bc == \"DQM\": # Quantile Delta Mapping method\n",
    "                    qdm_t2m = sdba.adjustment.QuantileDeltaMapping.train(ref = t2m_baseline, hist = t2m_model_ssp.sel(time = baseline_period), kind = \"+\", group=\"time.month\")\n",
    "                    t2m_model_ssp_bc = qdm_t2m.adjust(t2m_model_ssp.sel(time = future_period), interp=\"nearest\", extrapolation=\"constant\")      \n",
    "\n",
    "                    qdm_pp  = sdba.adjustment.QuantileDeltaMapping.train(ref = pp_baseline, hist = pp_model_ssp.sel(time  = baseline_period), kind = \"*\", group=\"time.month\")\n",
    "                    pp_model_ssp_bc  = qdm_pp.adjust(pp_model_ssp.sel(time  = future_period), interp=\"nearest\", extrapolation=\"constant\")\n",
    "\n",
    "                if bc == \"MBC\": # Npdf Transform method (xclim.readthedocs.io/en/stable/notebooks/sdba.html)\n",
    "                    \n",
    "                   # a) Perform an initial univariate adjustment\n",
    "                    qdm_t2m = sdba.QuantileDeltaMapping.train(t2m_baseline, t2m_model_ssp.sel(time = baseline_period), nquantiles = 20, kind = \"+\", group = \"time\")\n",
    "                    scen_hist_t2m = qdm_t2m.adjust(t2m_model_ssp.sel(time = baseline_period))\n",
    "                    scen_ssp_t2m  = qdm_t2m.adjust(t2m_model_ssp.sel(time = future_period))\n",
    "                    \n",
    "                    qdm_pp = sdba.QuantileDeltaMapping.train(pp_baseline, pp_model_ssp.sel(time = baseline_period), nquantiles = 20, kind = \"*\", group = \"time\")\n",
    "                    scen_hist_pp = qdm_pp.adjust(pp_model_ssp.sel(time = baseline_period))\n",
    "                    scen_ssp_pp  = qdm_pp.adjust(pp_model_ssp.sel(time = future_period))\n",
    "\n",
    "                    dref      = xr.Dataset(dict(tas = t2m_baseline, pr = pp_baseline))\n",
    "                    scen_hist = xr.Dataset(dict(tas = scen_hist_t2m, pr = scen_hist_pp))\n",
    "                    scen_ssp  = xr.Dataset(dict(tas = scen_ssp_t2m, pr = scen_ssp_pp))\n",
    "                    scen_hist[\"time\"] = dref.time # correct date (15 -> 01)\n",
    "                    \n",
    "                    # b) Stack the variables to multivariate arrays and standardize them\n",
    "                    ref   = sdba.processing.stack_variables(dref) # Stack the variables (tas and pr)\n",
    "                    scenh = sdba.processing.stack_variables(scen_hist)\n",
    "                    scens = sdba.processing.stack_variables(scen_ssp)\n",
    "            \n",
    "                    ref, _, _          = sdba.processing.standardize(ref) # Standardize\n",
    "                    allsim, savg, sstd = sdba.processing.standardize(xr.concat((scenh, scens), \"time\"))\n",
    "\n",
    "                    hist = allsim.sel(time = scenh.time)\n",
    "                    sim  = allsim.sel(time = scens.time)\n",
    "                                        \n",
    "                    # c) Perform the N-dimensional probability density function transform\n",
    "                    out = sdba.adjustment.NpdfTransform.adjust(ref, hist, sim, base=sdba.QuantileDeltaMapping, \n",
    "                                                               base_kws={\"nquantiles\": 20, \"group\": \"time.month\"}, n_iter=20, n_escore=1000)  \n",
    "                    model_ssp_bc = sdba.processing.reordering(out, scens, group=\"time.month\")\n",
    "                    model_ssp_bc = sdba.processing.unstack_variables(model_ssp_bc)\n",
    "                    \n",
    "                    # d) Restoring the trend\n",
    "                    model_ssp_bc = sdba.processing.reordering(sim, scens, group=\"time\")\n",
    "                    model_ssp_bc = sdba.processing.unstack_variables(model_ssp_bc)\n",
    "\n",
    "                    t2m_model_ssp_bc = model_ssp_bc.tas\n",
    "                    pp_model_ssp_bc  = model_ssp_bc.pr.clip(min = 0)\n",
    "                    \n",
    "                # save file\n",
    "                rid = \"_{}_{}_{}_{}\".format(file_id, gcm, ssp, bc)\n",
    "\n",
    "                t2m_model_ssp_bc = t2m_model_ssp_bc.rename(\"tas\").transpose('time', 'lat', 'lon')\n",
    "                t2m_model_ssp_bc.to_netcdf(\"/home/rooda/OGGM_results/Future_climate_bc/T2M\" + rid + \".nc\", encoding = encode_t2m)\n",
    "                pp_model_ssp_bc  = pp_model_ssp_bc.rename(\"pr\").transpose('time', 'lat', 'lon')\n",
    "                pp_model_ssp_bc.to_netcdf(\"/home/rooda/OGGM_results/Future_climate_bc/PP\" + rid + \".nc\", encoding = encode_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5553b4-9d5e-4164-97e0-c39f76407a48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
